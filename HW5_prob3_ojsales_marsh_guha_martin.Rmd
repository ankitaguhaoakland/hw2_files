---
title: "HW5 - Problem 3 - Orange Juice classification"
author: "misken"
date: "March 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 3 - Predicting orange juice purchases

The dataset is available as part of the ISLR package. You can see the
documentation for that package or the following link describes the OJ
dataset - https://rdrr.io/cran/ISLR/man/OJ.html.

**SUGGESTION**: See the material available in Downloads_StatModels2 from the
session on classification models in R. In particular, the folder on
logistic regression and the example in the folder intro_class_HR/ will
be useful.

## Data prep

We'll do a little data prep to set things up so that we are trying to
predict whether or not the customer purchased Minute Maid (vs Citrus Hill.)
Just run the following chunks to load the dataset, do some data prep and
then partition the data into training and test sets.

```{r loaddata}
ojsales <- (ISLR::OJ)
```

Loading a few libraries
```{r}
library(dplyr)
library(ggplot2)
library(useful)
library(coefplot)
library(caret)
library(lattice)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(class)
library(randomForest)
library(reshape2)
```

Clean up the storeid related fields. Drop Store7 field.

```{r}
ojsalesdf <- ojsales[ojsales$StoreID != "7",]

# Omitting NA's in the dataframe
na.omit(ojsalesdf)
```


```{r factors}
ojsalesdf$StoreID <- as.factor(ojsalesdf$StoreID)

# Create a new variable to act as the response variable.
ojsalesdf$MM <- as.factor(ifelse(ojsalesdf$Purchase=="MM",1,0))
```

Now we'll just take a subset of the columns as there are a few that contain the
same information. Remember, the new column `MM` is the one we are trying to
predict.

```{r subset}
ojsalesdf_subset <- ojsalesdf[, c(19, 3:13, 15:17)]
```

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 10% of
the full dataset.

```{r partition}
set.seed(167)
sample_size <- ceiling(0.10 * nrow(ojsalesdf))
testrecs <- sample(nrow(ojsalesdf_subset),sample_size)
ojsalesdf_test <- ojsalesdf_subset[testrecs,]
ojsalesdf_train <- ojsalesdf_subset[-testrecs,]  # Negative in front of vector means "not in"
#rm(ojsalesdf_subset, ojsalesdf) # No sense keeping a copy of the entire dataset around
```

## Your job

# EDA

Let's check if we really got rid of the StoreID 7 from our dataframe. Yup, we did! 
```{r}
head(ojsalesdf_train)
```

Let's check our dataframe type now
```{r}
str(ojsalesdf_train)
```

Let's take a look at the Summary of our dataframe 
```{r}
summary(ojsalesdf_train)
```

Let's try to fit our Model using Logistic Regression.

# Model 1: Logistic Regression
```{r}
MM_LogM1 <- glm(MM ~  .,
                    data=ojsalesdf_train, family=binomial(link="logit"))

summary(MM_LogM1)
```

Looks like our Model has fitted well, since the Residual deviance value is less than the value of Null deviance. But there are some of our co-efficients in the Model with NA values and for obvious reasons they does not look like to be contributing for our Model significance, let's remove them from our Model and fit our Model and check if the Residual deviance and the Null deviance makes any difference.

```{r}
MM_LogM1 <- glm(MM ~  . - SalePriceMM - ListPriceDiff - SalePriceCH - PriceDiff,
                    data=ojsalesdf_train, family=binomial(link="logit"))

summary(MM_LogM1)
```
Okay, so after removal of the coefficient values from our Model with the NA values, the Model performance remains unchanged, as assumed.

Converting probabilities to 0 and 1 for predictions.
```{r}
yhat_LogM1 <- (MM_LogM1$fit > 0.5) * 1
```

Putting the fitted and the predicted values into a dataframe for ease of analysis
```{r}
MM_fit_predictions <- data.frame(MM_LogM1$y, yhat_LogM1)
names(MM_fit_predictions) <- c("yact","yhat_LogM1")
```

## Creating Confusion Matrix
```{r}
# Confusion Matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1))
```


```{r}
cm_LogM1 <- confusionMatrix(MM_fit_predictions$yhat_LogM1, MM_fit_predictions$yact, 
                            positive = "1")
cm_LogM1
```
Looks like our Model has an Accuracy of 82% and the Sensitivity or the Probability of True Positive is 80% at the 95% confidence interval.

Well, looks like our Logistic Regression data is well fit in between the probability of 0 and 1. 

Visualizing the rate of purchase of MM vs CH. We got an almost ideal distribution scores with the scores of negative instances (0) to the left represented by the Orange Line and the distribution of positive instances (1) to the right represented by the dotted Green Line. This Orange solid line represents the distribution of the customer not buying MM or in other words the distribution of the customer buying CH and the dotted Green line represents the distributin of the customer buying MM or in other words the distribution of the customers not buying CH. 

And this distribution is actually correct as we can see from the Confusion Matrix as well.
```{r}
ggplot(MM_fit_predictions, aes(x=yhat_LogM1, y=yact)) + geom_point() + 
stat_smooth(method="glm", family="binomial", se=FALSE)

# Double Density Plot
ggplot(ojsalesdf_train, aes(x=yhat_LogM1, color=MM, linetype=MM)) + geom_density()
```


## Predictions on Test Data for Model 1: Logistic Regression
```{r}
prob_LogM1 <- predict(MM_LogM1, newdata=ojsalesdf_test, type="response")
pred_LogM1 <- (prob_LogM1 > 0.5) * 1
head(pred_LogM1)
```


# Model 2: Simple Decision Tree
```{r}
MM_tree <- rpart(MM ~ ., data=ojsalesdf_train, method="class")

# Visualize the tree with rpart.plot
rpart.plot(MM_tree)
```

## Creating Confusion Matrix
```{r}
MM_fit_predictions$yhat_tree <- predict(MM_tree, type = "class")

# Confusion Matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_tree)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_tree))
```

```{r}
cm_tree <- confusionMatrix(MM_fit_predictions$yhat_tree, MM_fit_predictions$yact, 
                            positive = "1")
cm_tree
```

## Predictions on Test Data for Model 2: Simple Decision Tree
```{r}
pred_tree <- predict(MM_tree, newdata=ojsalesdf_test, type="class")
head(pred_tree)
```


You should build at least two classification models to try to predict MM.
Our error metric will be overall accuracy.

Obviously, `ojsales_train` is your training dataset. After fitting each
model, use the `caret::confusionMatrix` function to create a confusion matrix
for each of the models based on the training data.

You should at least try the following two techniques:
- logistic regression
- a simple decision tree

**HACKER EXTRA:** Try additional techniques such as random forest, k-nearest 
neighbor or others.

# Model 3: Random Forest
```{r}
rand_forest <- randomForest(MM ~ ., data=ojsalesdf_train, method = "class")
rand_forest
```

## Creating Confusion Matrix
```{r}
MM_fit_predictions$yhat_rf <- predict(rand_forest, type = "class")

# Confusion matrix
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_rf)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_rf))
```

```{r}
cm_rf <- confusionMatrix(MM_fit_predictions$yhat_rf, MM_fit_predictions$yact, 
                            positive = "1")
cm_rf
```

## Predictions on Test Data for Model 3: Random Forest
```{r}
pred_rf <- predict(rand_forest, newdata=ojsalesdf_test, type="class")
head(pred_rf)
```


# Model 4: k-nearest Neighbor
```{r}
# Function to normalize data
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
```

Checking the characteristics of our original dataframe
```{r}
str(ojsalesdf_subset)
```

Rescaling all of our dataframes for building prediction Models with k-nearest neighbor Classification. Since our total number of observations = 714. Taking approximate value of the square root of the number of total observations. That is Square Root of 714 = 26.7. 

```{r}
# Scaling the Dataframes
num.vars_subset <- sapply(ojsalesdf_subset, is.numeric)
ojsalesdf_subset[num.vars_subset] <- lapply(ojsalesdf_subset[num.vars_subset], scale)

num.vars_train <- sapply(ojsalesdf_train, is.numeric)
ojsalesdf_train[num.vars_train] <- lapply(ojsalesdf_train[num.vars_train], scale)

num.vars_test <- sapply(ojsalesdf_test, is.numeric)
ojsalesdf_test[num.vars_test] <- lapply(ojsalesdf_test[num.vars_test], scale)

# Prediction with k = 27
pred_knn27 <- knn(ojsalesdf_train, ojsalesdf_test, ojsalesdf_train$MM, k=27, prob = TRUE)
head(pred_knn27)
```

## Creating Confusion Matrix
```{r}
pcol1 <- as.character(as.numeric(ojsalesdf_train$MM))
pairs(ojsalesdf_test, pch = pcol1, col = c("red", "green3")
  [(ojsalesdf_train$MM != pred_knn27)+1])
```


Then use the `predict()` function to make classification predictions on the
test dataset and use `caret::confusionMatrix` to create a confusion matrix
for each of the models for the predictions. 

## Summarizing 

Now let’s gather the three sets of prediction along with the actual values into its own dataframe.
```{r}
pred_results <- as.data.frame(cbind(pred_LogM1, 
                                    pred_tree, pred_rf, pred_knn27, ojsalesdf_test$MM))

names(pred_results) <- c("pred_LogM1", "pred_tree", "pred_rf", "pred_knn27", "test_MM")
head(pred_results)
```

Hmmm, looks like the values are the level numbers. Let’s just adjust to match our logistic regression predictions for the rest of our Models.

```{r}
pred_results$pred_tree <- pred_results$pred_tree - 1
pred_results$pred_rf <- pred_results$pred_rf - 1
pred_results$pred_knn27 <- pred_results$pred_knn27 - 1
pred_results$test_MM <- pred_results$test_MM - 1
head(pred_results)
```

Now let’s just create a confusion matrix for each model and the actual values from the ojsalesdf_test dataset and compare their performance.

```{r}
cm_predLogM1 <- confusionMatrix(pred_results$pred_LogM1, 
                                 pred_results$test_MM, 
                                 positive = "1")

cm_predtree <- confusionMatrix(pred_results$pred_tree, 
                                pred_results$test_MM, 
                                positive = "1")

cm_predrf <- confusionMatrix(pred_results$pred_rf, 
                                pred_results$test_MM, 
                                positive = "1")

cm_predknn27 <- confusionMatrix(pred_results$pred_knn27, 
                                pred_results$test_MM, 
                                positive = "1")

```

## Confusion Matrix for Predicted Model 1: Logistic Regression
```{r}
cm_predLogM1
```

## Confusion Matrix for Predicted Model 2: Simple Decision Tree
```{r}
cm_predtree
```

## Confusion Matrix for Predicted Model 3: Random Forest
```{r}
cm_predrf
```

## Confusion Matrix for Predicted Model 4: k- Nearest Neighbor
```{r}
cm_predknn27
```

## Accuracy & Sensitivity for all the Fitted Models
```{r}
# Model 1: Logistic Regression
sprintf("Model 1 - Logistic Regression: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_LogM1$overall["Accuracy"],
        cm_LogM1$byClass["Sensitivity"])
```

```{r}
# Model 2: Simple Decision Tree
sprintf("Model 1 - Simple Decision Tree: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_tree$overall["Accuracy"],
        cm_tree$byClass["Sensitivity"])
```

```{r}
# Model 3: Random Forest
sprintf("Model 1 - Random Forest: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_rf$overall["Accuracy"],
        cm_rf$byClass["Sensitivity"])
```


## Accuracy & Sensitivity for all the Predicted Models
```{r}
# Model 1: Logistic Regression
sprintf("Model 1 - Logistic Regression: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_predLogM1$overall["Accuracy"],
        cm_predLogM1$byClass["Sensitivity"])
```

```{r}
# Model 2: Simple Decision Tree
sprintf("Model 1 - Simple Decision Tree: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_predtree$overall["Accuracy"],
        cm_predtree$byClass["Sensitivity"])
```

```{r}
# Model 3: Random Forest
sprintf("Model 1 - Random Forest: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_predrf$overall["Accuracy"],
        cm_predrf$byClass["Sensitivity"])
```

```{r}
# Model 4: k-Nearest Neighbor
sprintf("Model 1 - k-Nearest Neighbor: Accuracy = %8.4f Sensitivity = %8.4f", 
        cm_predknn27$overall["Accuracy"],
        cm_predknn27$byClass["Sensitivity"])
```

Summarize your results. 

- Which technique performed the best in terms of overall accuracy? 
+ Out of all the 4 Models, looks like k-Nearest Neighbor performed the best in terms of overall accuracy. 
But, since the Model accuracy or sensitivity would differ with different values of k, therefore would consider our traditional **Logistic Regression** to have performed best in terms of overall accuracy.

- Which technique had the best sensitivity score?
+ Out of all the 4 Models, looks like k-Nearest Neighbor performed the best in terms of overall sensitivity score. 
But, since the Model accuracy or sensitivity would differ with different values of k, therefore would consider our traditional **Logistic Regression** to have performed best in terms of overall sensitivity score.

- How did accuracy differ for the training and test datasets for each model?
+ For almost all the Models there lies a significant difference in Accuracy and Sensitivity for all the Models in their training and test data as seen from the above summarized data. 

- Is their any evidence of overfitting?
+ Overall, there looks like there might be an overfitting with our k-Nearest Neighbor Model, becuase the value depends largely on the selected value of k for the purpose of predicting the value. While for all the rest of the Models, especially the Random Forest and the Simple Decision Tree they appeared to be underfit.